{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_PATH = 'data/MovieSummaries/'\n",
    "character_metadata = pd.read_csv(DATA_PATH + 'character.metadata.tsv', \n",
    "                                 sep='\\t', \n",
    "                                 names= [\n",
    "                                     'Wikipedia movie ID',\n",
    "                                     'Freebase movie ID',\n",
    "                                     'Movie release date',\n",
    "                                     'Character name',\n",
    "                                     'Actor date of birth',\n",
    "                                     'Actor gender',\n",
    "                                     'Actor height (in meters)',\n",
    "                                     'Actor ethnicity (Freebase ID)',\n",
    "                                     'Actor name',\n",
    "                                     'Actor age at movie release',\n",
    "                                     'Freebase character/actor map ID',\n",
    "                                     'Freebase character ID',\n",
    "                                     'Freebase actor ID'\n",
    "                                 ]\n",
    "                                 )\n",
    "\n",
    "movie_metadata = pd.read_csv(DATA_PATH + 'movie.metadata.tsv', sep='\\t', header=0,\n",
    "                             names=['Wikipedia movie ID',\n",
    "                                         'Freebase movie ID',\n",
    "                                         'Movie name',\n",
    "                                         'Movie release date',\n",
    "                                         'Movie box office revenue',\n",
    "                                         'Movie runtime',\n",
    "                                         'Movie languages (Freebase ID:name tuples)',\n",
    "                                         'Movie countries (Freebase ID:name tuples)',\n",
    "                                         'Movie genres (Freebase ID:name tuples)'\n",
    "                                         ])\n",
    "\n",
    "plot_summaries = pd.read_csv(DATA_PATH + 'plot_summaries.txt', sep='\\t', names=[\n",
    "    'Wikipedia movie ID',\n",
    "    'Summary'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a DataFrame\n",
    "texts = plot_summaries['Summary'].tolist()\n",
    "if os.path.exists('data/embedded_summaries.csv'):\n",
    "    embedding_df = pd.read_csv('data/embedded_summaries.csv')\n",
    "else:\n",
    "    embedding_df = pd.DataFrame(columns=['Summary', 'embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\x0b\", \" \")\n",
    "    text = text.replace(\"\\x0c\", \" \")\n",
    "    try:\n",
    "        embedding = openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n",
    "\n",
    "        if embedding is None:\n",
    "            print(f\"Failed to process text: {text}. Error: embedding is None\")\n",
    "            return None\n",
    "        return embedding\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"Failed to process text: {text}. Error: {str(e)}\")\n",
    "        time.sleep(60)\n",
    "        return None\n",
    "\n",
    "def process_texts(start, end):\n",
    "    global embedding_df\n",
    "    for i in range(start, end):\n",
    "        if i >= len(texts):\n",
    "            break\n",
    "        if (i-start) % 500 == 0 and i - start > 0:\n",
    "            print(f\"Processing {i-start}th text\")\n",
    "        text = texts[i]\n",
    "\n",
    "        # also check that the result is not nan\n",
    "        if embedding_df[embedding_df['Summary']==text].shape[0] > 0 and not embedding_df[embedding_df['Summary']==text]['embedding'].isna().iloc[0]:\n",
    "            continue\n",
    "        if embedding_df[embedding_df['Summary']==text].shape[0] > 0:\n",
    "            embedding_df.loc[embedding_df['Summary']==text, 'embedding'] = get_embedding(text)\n",
    "        else:\n",
    "            embedding_df = embedding_df.append({'Summary': text, 'embedding': get_embedding(text)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "chunk_size = len(texts) // num_cores\n",
    "chunks = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_cores)]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = [executor.submit(process_texts, start, end) for start, end in chunks]\n",
    "    concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.to_csv('data/embedded_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv('data/embedded_summaries.csv')\n",
    "combined_plot_summaries = pd.merge(plot_summaries, embeddings, on='Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_embedding_to_numpy(embedding_str):\n",
    "    return np.array([float(x) for x in embedding_str[1:-1].split(',')])\n",
    "\n",
    "tqdm.pandas(desc=\"Converting embeddings\")\n",
    "combined_plot_summaries['embedding'] = combined_plot_summaries['embedding'].progress_apply(convert_embedding_to_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now sample 1000 movie embeddings and remove all movies that are less than 0.2 similar to the mean of the sample\n",
    "# We do this to remove outliers\n",
    "sample = np.random.choice(combined_plot_summaries.index, 1000)\n",
    "sample_embeddings = combined_plot_summaries.loc[sample]['embedding'].values\n",
    "sample_mean = np.mean(sample_embeddings, axis=0)\n",
    "\n",
    "# We now remove all movies that are less than 0.2 similar to the mean of the sample\n",
    "\n",
    "def remove_outliers(embedding):\n",
    "    return np.dot(embedding, sample_mean) > 0.2\n",
    "\n",
    "combined_plot_summaries = combined_plot_summaries[combined_plot_summaries['embedding'].apply(remove_outliers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings to disk in a efficient format with both embeddings and Wikipedia movie ID\n",
    "np.save('data/embeddings.npy', combined_plot_summaries[['Wikipedia movie ID', 'embedding']].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
